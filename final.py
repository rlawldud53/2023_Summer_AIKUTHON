# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OJGvLLqSEf_cKp0HRWaKA52S9tjYLeAT
"""

# Korean Ver Import (local 에 설치 필요)
# koBERT tokenizer용 라이브러리
!pip install mxnet
!pip install gluonnlp==0.8.0
!pip install tqdm pandas
!pip install sentencepiece
!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf' # koBERT tokenizer

!pip install torch
!pip install transformers
!pip install datasets

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from os import path
from datetime import datetime

# for koBERT
from kobert_tokenizer import KoBERTTokenizer
from transformers.optimization import AdamW, get_cosine_schedule_with_warmup
from tqdm import tqdm
import sentencepiece

import numpy as np
import math

tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
model = BertForSequenceClassification.from_pretrained('monologg/kobert',num_labels=7)

"""# Import Data"""

from google.colab import drive
drive.mount('/content/drive')

# Load CSV.
#df = pd.read_csv('/content/drive/MyDrive/new_train.csv', header=None, names=['label', 'lyric'])
df = pd.read_csv('/content/drive/MyDrive/D2D/2023_SUMMER_AIKUTHON/new_train.csv', header=None, names=['label', 'lyric'])
df.head()

# Split datas into frames.

labels = list(map(int,df['label'].tolist()[1:110000]))
lyrics = df['lyric'][1:110000].tolist()

# label_dict = {label: i for i, label in enumerate(set(labels))} # 필요시 삭제
# labels = [label_dict[label] for label in labels]

'''
prev_model_path = 'lyrics_model'

if path.exists(prev_model_path):
    tokenizer = BertTokenizer.from_pretrained(prev_model_path)
    model = BertForSequenceClassification.from_pretrained(prev_model_path)
    print("Model Loaded.", model)
else:
'''

print(labels)
#print(lyrics)

label_dict = {label: i for i, label in enumerate(set(labels))} #

labels = [label_dict[label] for label in labels] #

def cls_preprocess(dataset):

  preprocessed = []

  for i in range(len(dataset)):
    dataset[i] = "[CLS]" + dataset[i] + "[SEP]"
    preprocessed.append(dataset[i])

  return preprocessed

cls_preprocess(lyrics)

'''
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_dict))
print("New model created.", model)
'''

# tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
# model = BertForSequenceClassification.from_pretrained('monologg/kobert')
'''
from kobert_transformers import get_kobert_model
from transformers import BertModel, DistilBertModel
model = get_kobert_model() # BertModel.from_pretrained("monologg/kobert")

from kobert_transformers import get_tokenizer
tokenizer = get_tokenizer()
'''
# tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')
# tokenizer.tokenize("[CLS] 한국어 모델을 공유합니다. [SEP]")

# encode lyrics

encoded_lyrics = tokenizer.batch_encode_plus(
    lyrics,
    add_special_tokens=True,
    padding='longest',
    truncation=True,
    max_length=512, # 수정
    return_attention_mask=True,
    return_tensors='pt'
)

from sklearn.preprocessing import OneHotEncoder

import torch.nn.functional as F

#one_hot_label_ = F.one_hot(torch.Tensor(labels, num_classes=7)

# lyrics class

class LyricsDataset(Dataset):
    def __init__(self, lyrics, labels, attention_masks):
        self.lyrics = lyrics
        self.labels = labels
        self.attention_masks = attention_masks
        self.num_classes = 7
        self.one_hot_labels = torch.zeros(len(labels), self.num_classes)
        for i, label in enumerate(self.labels):
              #print(i,label)
              self.one_hot_labels[i, label] = 1.0

    def __len__(self):
        return len(self.lyrics)

    def __getitem__(self, idx):
        return {
            'lyric': self.lyrics[idx],
            'label': self.one_hot_labels[idx],
            'attention_mask': self.attention_masks[idx],
            'gt_label': self.labels[idx]
        }


        num_classes = len(set(labels))
        self.one_hot_labels = torch.zeros(len(labels), num_classes)
        for i, label in enumerate(labels):
            self.one_hot_labels[i, label] = 1.0

# Create dataset

dataset = LyricsDataset(
    lyrics=encoded_lyrics['input_ids'],
    labels=labels,
    attention_masks=encoded_lyrics['attention_mask'],
)


# Split dataset

def dataset_split(dataset, ratio):
    train_size = int(ratio * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    return train_dataset, val_dataset

# Dataloader config
batch_size = 32

train_dataset, val_dataset = dataset_split(dataset, 0.8)

train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# for data in train_dataloader:
  # print(data['lyric'])

'''
while True:
    model.train()
    total_loss = 0

    for batch in dataloader:
        lyrics = batch['lyric'].to(device)
        labels = batch['label'].to(device)
        attention_masks = batch['attention_mask'].to(device)

        optimizer.zero_grad()
        outputs = model(
            lyrics,
            token_type_ids=None,
            attention_mask=attention_masks,
            labels=labels
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

    avg_loss = total_loss / len(dataloader)
    print(f'Epoch {epoch + 1} - Loss: {avg_loss}', datetime.now().strftime("%H:%M:%S"))

    # Save Model

    model.save_pretrained('lyrics_model')
    tokenizer.save_pretrained('lyrics_model')

    epoch += 1
  '''

# tokenizer.model_max_length

import logging

logger = logging.getLogger()  ###진행 과정에서 로깅 포인트가 발생할 경우 로깅을 하기 위한 코드입니다. 수정하실 필요 없습니다
logger.setLevel(logging.INFO)  ###진행 과정에서 로깅 포인트가 발생할 경우 로깅을 하기 위한 코드입니다. 수정하실 필요 없습니다

# Config
epochs = 3
warmup_ratio = 0.1
lr = 2e-5
grad_clip = 1.0
train_log_interval = 500 # train 이 100번 이루어질 때마다 logging# validation_interval = 1000
save_interval = 1000 ##save point는 1000번의 train


# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

# scheduler
data_len = len(train_dataloader)
num_train_steps = int(data_len / batch_size * epochs)
num_warmup_steps = int(num_train_steps * warmup_ratio)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)

# Train

def model_train(model, optimizer, scheduler, train_dataloader, device):
    model.to(device)  # 모델 학습을 설정된 device (CPU, cuda) 위에서 진행하도록 설정

    model.train() # 모델을 학습 모드로 전환
    loss_list_between_log_interval = []

    for epoch_id in range(epochs):

        for step_index, batch_data in tqdm(enumerate(train_dataloader), f"[TRAIN] Epoch:{epoch_id+1}", total=len(train_dataloader)):
                global_step = len(train_dataloader) * epoch_id + step_index + 1

                # Add a condition to break the loop if we've gone through all data points
                if step_index * batch_size >= len(dataset):
                  continue

                optimizer.zero_grad()
                lyrics = batch_data['lyric']
                labels = batch_data['label']
                attention_masks = batch_data['attention_mask']

                # 모델의 input들을 device(GPU)와 호환되는 tensor로 바꿔줍니다.
                lyrics = lyrics.to(device)
                labels = labels.to(device)
                attention_masks = attention_masks.to(device)
                # enocded_lyric = labels.to(deivce) # 수정

                model_outputs = model(
                    lyrics, token_type_ids=None, attention_mask=attention_masks, labels=labels
                    )

                loss = model_outputs.loss
                loss.backward()

                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
                optimizer.step()
                scheduler.step()

                # for logging
                loss_list_between_log_interval.append(model_outputs.loss.item())

                #if global_step % train_log_interval == 0:

        mean_loss = np.mean(loss_list_between_log_interval)

        # 콘솔에 출력되는 부분
        logger.info(
            f"EP:{epoch_id} global_step:{global_step} "
            f"loss:{mean_loss:.4f} perplexity:{math.exp(mean_loss):.4f}"
        )

        loss_list_between_log_interval.clear()

        # if global_step % validation_interval == 0:
        # dev_loss = _validate(model, val_dataloader, device, logger, global_step)

        # 모델 저장 코드
        #state_dict = model.state_dict()
        # model_path = os.path.join('/content/drive/MyDrive/ML/my_AIKU/DeepIntoDeep/checkpoints', f"kobert_epoch_{epoch_id}.pth")
        #model_path = os.path.join('/content/drive/MyDrive/D2D/2023_SUMMER_AIKUTHON/checkpoints', f"kobert_epoch_{epoch_id}.pth")
        # logger.info(f"global_step: {global_step} model saved at {model_path}")
        #torch.save(state_dict, model_path)

    return model

# 실제 훈련
model_v1 = model_train(model, optimizer, scheduler, train_dataloader, device)

"""# Evaluation"""

# inference

def model_inference(model, val_dataloader):
  predictions = []
  groundtruths = []

  model.eval()

  for batch_data in tqdm(val_dataloader):
    with torch.no_grad():
                lyrics = batch_data['lyric']
                labels = batch_data['label']
                attention_masks = batch_data['attention_mask']
                gt_labels=batch_data['gt_label']

                lyrics = lyrics.to(device)
                labels = labels.to(device)
                attention_masks = attention_masks.to(device)
                # enocded_lyric = labels.to(deivce) # 수정

                outputs = model(
                    lyrics, token_type_ids=None, attention_mask=attention_masks, labels=labels
                    )


                logits = outputs.logits
                #print(logits)
                # Predict Genre (숫자값)
                predicted_labels = torch.argmax(logits, dim=1)
                predictions.append(predicted_labels)
                groundtruths.append(gt_labels)
                '''
                int_to_genre = {0:'발라드',
                                1:'록/메탈',
                                2:'댄스',
                                4:'POP',
                                5:'R&B/Soul',
                                7: '랩/힙합',
                                16: '성인가요/트로트'}
                '''

  return predictions, groundtruths

# 모델의 예측값

pred, gt = model_inference(model_v1, val_dataloader)
# pred = pred.tolist()
# print(pred)
# print(len(val_dataloader))

output=torch.cat(pred)

fin_pred = output.tolist()

fin_pred

!pip install evaluate

gt=torch.cat(gt).tolist()

gt

import evaluate

accuracy_metric = evaluate.load("accuracy")
results = accuracy_metric.compute(references=gt, predictions=fin_pred)
print(results)

'''
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)
'''

"""# Test (Submission)"""

test_df = pd.read_csv('/content/drive/MyDrive/D2D/2023_SUMMER_AIKUTHON/test.csv', header=None, names=['label', 'lyric'])

test_lyrics = test_df['lyric'][1:].tolist()

cls_preprocess(test_lyrics)

encoded_lyrics = tokenizer.batch_encode_plus(
    test_lyrics,
    add_special_tokens=True,
    padding='longest',
    truncation=True,
    max_length=512, # 수정
    return_attention_mask=True,
    return_tensors='pt'
)

class LyricsDataset2(Dataset):
    def __init__(self, lyrics, attention_masks):
        self.lyrics = lyrics
        self.attention_masks = attention_masks
        self.num_classes = 7

    def __len__(self):
        return len(self.lyrics)

    def __getitem__(self, idx):
        return {
            'lyric': self.lyrics[idx],
            'attention_mask': self.attention_masks[idx],
        }


        num_classes = len(set(labels))


# Create dataset

dataset = LyricsDataset2(
    lyrics=encoded_lyrics['input_ids'],
    attention_masks=encoded_lyrics['attention_mask'],
)

len(dataset)

test_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

len(test_dataloader)

def model_inferenc2(model, val_dataloader):
  predictions = []
 # groundtruths = []

  model.eval()

  for batch_data in tqdm(val_dataloader):
    with torch.no_grad():
                lyrics = batch_data['lyric']
  #              labels = batch_data['label']
                attention_masks = batch_data['attention_mask']
  #             gt_labels=batch_data['gt_label']

                lyrics = lyrics.to(device)
  #              labels = labels.to(device)
                attention_masks = attention_masks.to(device)
                # enocded_lyric = labels.to(deivce) # 수정

                outputs = model(
                    lyrics, token_type_ids=None, attention_mask=attention_masks
                    )


                logits = outputs.logits
                #print(logits)
                # Predict Genre (숫자값)
                predicted_labels = torch.argmax(logits, dim=1)
                predictions.append(predicted_labels)
  #              groundtruths.append(gt_labels)
                '''
                int_to_genre = {0:'발라드',
                                1:'록/메탈',
                                2:'댄스',
                                4:'POP',
                                5:'R&B/Soul',
                                7: '랩/힙합',
                                16: '성인가요/트로트'}
                '''

  return predictions

predictions = model_inferenc2(model_v1, test_dataloader)

len(predictions)

predictions

predictions[-1]

predictions2 = (torch.concat(predictions)).tolist()

predictions2

len(predictions2)

predictions3 = []
for i in tqdm(range(len(predictions2))):
    data = predictions2[i]
    if data == 3 : predictions3.append('4')
    elif data == 4 : predictions3.append('5')
    elif data == 5 : predictions3.append('7')
    elif data == 6 : predictions3.append('16')
    elif data == 0 : predictions3.append('0')
    elif data == 1 : predictions3.append('1')
    elif data == 2 : predictions3.append('2')
    else : predictions3.append(str(data))

predictions3

test_df2 = pd.read_csv('/content/drive/MyDrive/D2D/2023_SUMMER_AIKUTHON/test.csv')

test_df = test_df2['Unnamed: 0']

test_df = test_df.to_frame()

len(test_df)

df_fin = test_df

# id
df_fin['id'] = test_df2[['Unnamed: 0']]

# genre
df_fin['genre'] = predictions3

#file
df_fin.to_csv('/content/drive/MyDrive/D2D/2023_SUMMER_AIKUTHON/result2.csv',sep=',', columns = ['id', 'genre'], index = False) # do not write index